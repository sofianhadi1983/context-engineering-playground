{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Evaluation Metrics\n\nLearn how to define, measure, and track prompt quality systematically.\n\n## What You'll Learn\n- Defining custom evaluation metrics\n- Automated quality assessment\n- Scoring systems\n- Tracking improvements over time"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from prompt_playground.client import create_client, send_prompt\nfrom prompt_playground.analysis import calculate_metrics, analyze_tone, extract_key_points\nimport pandas as pd\nfrom rich import print as rprint\n\nclient = create_client()\nrprint('[green]✓[/green] Ready for evaluation')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Built-in Metrics"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = send_prompt(\n    prompt='Explain neural networks in simple terms.',\n    client=client\n)\n\nmetrics = calculate_metrics(response)\nrprint('\\n[bold]Quantitative Metrics:[/bold]')\nfor key in ['word_count', 'char_count', 'sentence_count', 'avg_sentence_length']:\n    rprint(f'{key}: {metrics[key]}')\n\ntone = analyze_tone(response['text'])\nrprint('\\n[bold]Qualitative Metrics:[/bold]')\nfor key in ['formality', 'complexity', 'perspective']:\n    rprint(f'{key}: {tone[key]}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Custom Evaluation Functions"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_clarity(text):\n    words = text.split()\n    avg_word_length = sum(len(w) for w in words) / len(words) if words else 0\n    score = 10 - min(avg_word_length - 4, 5)\n    return max(0, min(10, score))\n\ndef evaluate_conciseness(response):\n    metrics = calculate_metrics(response)\n    words = metrics['word_count']\n    if words < 50:\n        return 10\n    elif words < 100:\n        return 7\n    elif words < 200:\n        return 5\n    else:\n        return 3\n\ndef evaluate_structure(text):\n    has_intro = any(word in text.lower()[:100] for word in ['intro', 'first', 'begin'])\n    has_conclusion = any(word in text.lower()[-100:] for word in ['conclu', 'summary', 'finally'])\n    paragraphs = len([p for p in text.split('\\n\\n') if p.strip()])\n    \n    score = 0\n    if has_intro:\n        score += 3\n    if has_conclusion:\n        score += 3\n    if paragraphs >= 2:\n        score += 4\n    \n    return score\n\nresponse = send_prompt(prompt='Explain photosynthesis.', client=client)\nrprint(f'Clarity: {evaluate_clarity(response[\"text\"])}/10')\nrprint(f'Conciseness: {evaluate_conciseness(response)}/10')\nrprint(f'Structure: {evaluate_structure(response[\"text\"])}/10')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Composite Scoring System"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ResponseEvaluator:\n    def __init__(self, weights=None):\n        self.weights = weights or {\n            'clarity': 0.3,\n            'conciseness': 0.2,\n            'structure': 0.2,\n            'tone_match': 0.3\n        }\n    \n    def evaluate(self, response, target_tone='formal'):\n        clarity = evaluate_clarity(response['text'])\n        conciseness = evaluate_conciseness(response)\n        structure = evaluate_structure(response['text'])\n        \n        tone = analyze_tone(response['text'])\n        tone_match = 10 if tone['formality'] == target_tone else 5\n        \n        scores = {\n            'clarity': clarity,\n            'conciseness': conciseness,\n            'structure': structure,\n            'tone_match': tone_match\n        }\n        \n        total = sum(scores[k] * self.weights[k] for k in scores)\n        \n        return {\n            'total_score': total,\n            'scores': scores,\n            'grade': self._get_grade(total)\n        }\n    \n    def _get_grade(self, score):\n        if score >= 9:\n            return 'A'\n        elif score >= 7:\n            return 'B'\n        elif score >= 5:\n            return 'C'\n        else:\n            return 'D'\n\nevaluator = ResponseEvaluator()\nresponse = send_prompt(prompt='Explain blockchain technology.', client=client)\nresult = evaluator.evaluate(response, target_tone='formal')\n\nrprint(f'\\n[bold]Overall Score: {result[\"total_score\"]:.1f}/10[/bold]')\nrprint(f'Grade: {result[\"grade\"]}')\nrprint('\\nBreakdown:')\nfor metric, score in result['scores'].items():\n    rprint(f'  {metric}: {score}/10')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## A/B Testing with Metrics"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prompts = [\n    'Explain quantum computing.',\n    'Explain quantum computing in simple, non-technical terms with real-world examples.'\n]\n\nresults = []\nfor i, prompt in enumerate(prompts):\n    response = send_prompt(prompt=prompt, client=client)\n    evaluation = evaluator.evaluate(response)\n    results.append({\n        'variant': chr(65+i),\n        'score': evaluation['total_score'],\n        'grade': evaluation['grade'],\n        **evaluation['scores']\n    })\n\ndf = pd.DataFrame(results)\ndisplay(df)\n\nwinner = df.loc[df['score'].idxmax()]\nrprint(f'\\n[green]Winner: Variant {winner[\"variant\"]} (Score: {winner[\"score\"]:.1f})[/green]')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Tracking Improvements"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["versions = [\n    'Explain AI.',\n    'Explain artificial intelligence in simple terms.',\n    'Explain artificial intelligence in simple terms with a real-world example.',\n]\n\nhistory = []\nfor i, prompt in enumerate(versions, 1):\n    response = send_prompt(prompt=prompt, client=client)\n    eval_result = evaluator.evaluate(response)\n    history.append({\n        'version': f'v{i}',\n        'prompt': prompt[:50] + '...',\n        'score': eval_result['total_score'],\n        'grade': eval_result['grade']\n    })\n\ndf = pd.DataFrame(history)\nrprint('\\n[bold]Iteration History:[/bold]')\ndisplay(df)\n\nimprovement = df['score'].iloc[-1] - df['score'].iloc[0]\nrprint(f'\\nImprovement: {improvement:+.1f} points')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Best Practices\n\n### 1. Define Clear Criteria"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["criteria = {\n    'max_words': 150,\n    'min_sentences': 3,\n    'required_tone': 'formal',\n    'must_include': ['example', 'benefit']\n}\n\ndef meets_criteria(response, criteria):\n    metrics = calculate_metrics(response)\n    tone = analyze_tone(response['text'])\n    text_lower = response['text'].lower()\n    \n    checks = {\n        'word_count': metrics['word_count'] <= criteria['max_words'],\n        'sentence_count': metrics['sentence_count'] >= criteria['min_sentences'],\n        'tone': tone['formality'] == criteria['required_tone'],\n        'keywords': all(kw in text_lower for kw in criteria['must_include'])\n    }\n    \n    return checks, all(checks.values())\n\nrprint('[green]✓[/green] Clear criteria defined')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Use Multiple Runs"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prompt = 'Explain cloud computing briefly.'\nruns = 3\nscores = []\n\nfor _ in range(runs):\n    response = send_prompt(prompt=prompt, temperature=0.7, client=client)\n    result = evaluator.evaluate(response)\n    scores.append(result['total_score'])\n\navg_score = sum(scores) / len(scores)\nstd_dev = (sum((s - avg_score) ** 2 for s in scores) / len(scores)) ** 0.5\n\nrprint(f'Average: {avg_score:.1f} ± {std_dev:.1f}')\nrprint(f'Scores: {scores}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Document Evaluation Logic"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluation_docs = {\n    'clarity': 'Scores based on average word length (simpler = higher)',\n    'conciseness': 'Word count thresholds: <50=10pts, <100=7pts, <200=5pts',\n    'structure': 'Checks for intro, conclusion, multiple paragraphs',\n    'tone_match': '10pts if matches target, 5pts otherwise'\n}\n\nfor metric, description in evaluation_docs.items():\n    rprint(f'{metric}: {description}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n\nYou've learned:\n- ✓ Using built-in metrics\n- ✓ Creating custom evaluation functions\n- ✓ Building composite scoring systems\n- ✓ Comparing prompt variants with metrics\n- ✓ Tracking improvements over iterations\n- ✓ Best practices for evaluation\n\n## Congratulations!\n\nYou've completed all example notebooks. You now have the tools to:\n- Create and test prompts systematically\n- Use templates for consistency\n- Compare variants objectively\n- Process prompts at scale\n- Evaluate quality metrics\n\nStart applying these techniques to your own use cases!"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
