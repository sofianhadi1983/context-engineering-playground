{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing for Prompts\n",
    "\n",
    "Learn how to systematically compare prompt variants to find the most effective approach.\n",
    "\n",
    "## What You'll Learn\n",
    "- Testing multiple prompt variants\n",
    "- Analyzing response differences\n",
    "- Statistical comparison methods\n",
    "- Best practices for A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_playground.client import create_client, send_prompt, send_batch\n",
    "from prompt_playground.analysis import compare_responses, calculate_metrics, analyze_tone, visualize_comparison\n",
    "from rich import print as rprint\n",
    "from rich.panel import Panel\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "client = create_client()\n",
    "console = Console()\n",
    "rprint(\"[green]✓[/green] Ready for A/B testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why A/B Test Prompts?\n",
    "\n",
    "A/B testing helps you:\n",
    "- Find the most effective prompt structure\n",
    "- Optimize for specific metrics (clarity, conciseness, tone)\n",
    "- Make data-driven decisions\n",
    "- Understand what works best for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic A/B Test\n",
    "\n",
    "Compare two prompt variants for the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"explaining recursion to beginners\"\n",
    "\n",
    "variant_a = \"Explain recursion in simple terms.\"\n",
    "variant_b = \"Explain recursion using a real-world analogy that a beginner programmer would understand.\"\n",
    "\n",
    "prompts = [variant_a, variant_b]\n",
    "responses = send_batch(prompts=prompts, client=client)\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    rprint(f\"\\n[bold cyan]Variant {chr(64+i)}:[/bold cyan]\")\n",
    "    rprint(Panel(response['text'], expand=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = compare_responses(responses)\n",
    "df['variant'] = ['A', 'B']\n",
    "\n",
    "display(df[['variant', 'word_count', 'sentence_count', 'output_tokens', 'estimated_cost']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tone Analysis Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(title=\"Tone Comparison\")\n",
    "table.add_column(\"Variant\", style=\"cyan\")\n",
    "table.add_column(\"Formality\", style=\"yellow\")\n",
    "table.add_column(\"Complexity\", style=\"green\")\n",
    "table.add_column(\"Perspective\", style=\"blue\")\n",
    "\n",
    "for i, response in enumerate(responses):\n",
    "    tone = analyze_tone(response['text'])\n",
    "    table.add_row(\n",
    "        chr(65+i),\n",
    "        tone['formality'],\n",
    "        tone['complexity'],\n",
    "        tone['perspective']\n",
    "    )\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_comparison(responses, metric='length')\n",
    "plt.show()\n",
    "\n",
    "fig = visualize_comparison(responses, metric='tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Variant Testing (A/B/C/D)\n",
    "\n",
    "Test multiple approaches simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = [\n",
    "    \"Write a product description for wireless headphones.\",\n",
    "    \"Write a compelling product description for wireless headphones that highlights key benefits.\",\n",
    "    \"Create a product description for wireless headphones. Focus on: sound quality, battery life, comfort. Use persuasive language.\",\n",
    "    \"You are a product copywriter. Write an engaging description for wireless headphones that would appeal to music enthusiasts.\"\n",
    "]\n",
    "\n",
    "responses = send_batch(prompts=variants, temperature=0.7, client=client)\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    metrics = calculate_metrics(response)\n",
    "    rprint(f\"\\n[bold]Variant {chr(64+i)}:[/bold] {metrics['word_count']} words, ${metrics['estimated_cost']:.6f}\")\n",
    "    rprint(response['text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Prompt Structure\n",
    "\n",
    "Compare different structural approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = [\n",
    "    \"List 3 benefits of exercise.\",\n",
    "    \n",
    "    \"\"\"Task: List benefits of exercise\n",
    "Format: Numbered list\n",
    "Count: 3 items\"\"\",\n",
    "    \n",
    "    \"\"\"Please list 3 key benefits of regular exercise.\n",
    "    \n",
    "For each benefit:\n",
    "1. State the benefit\n",
    "2. Explain why it matters\"\"\"\n",
    "]\n",
    "\n",
    "responses = send_batch(prompts=structures, temperature=0.3, client=client)\n",
    "df = compare_responses(responses)\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    rprint(f\"\\n[bold cyan]Structure {i}:[/bold cyan]\")\n",
    "    rprint(Panel(response['text'], expand=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Temperature Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"Write a creative tagline for an eco-friendly water bottle.\"\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "\n",
    "results = []\n",
    "for temp in temperatures:\n",
    "    response = send_prompt(prompt=base_prompt, temperature=temp, client=client)\n",
    "    results.append({\n",
    "        'temperature': temp,\n",
    "        'response': response['text'],\n",
    "        'tokens': response['output_tokens']\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    rprint(f\"\\n[cyan]Temperature {result['temperature']}:[/cyan]\")\n",
    "    rprint(result['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Test One Variable at a Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"Explain photosynthesis.\"\n",
    "\n",
    "test_specificity = [\n",
    "    base,\n",
    "    \"Explain photosynthesis in simple terms.\",\n",
    "]\n",
    "\n",
    "test_audience = [\n",
    "    \"Explain photosynthesis in simple terms.\",\n",
    "    \"Explain photosynthesis in simple terms to a 5th grader.\",\n",
    "]\n",
    "\n",
    "rprint(\"[green]✓[/green] Testing specificity first, then audience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use Consistent Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\n",
    "    'temperature': 0.5,\n",
    "    'max_tokens': 200,\n",
    "    'client': client\n",
    "}\n",
    "\n",
    "variant_1 = send_prompt(prompt=\"Variant 1...\", **test_params)\n",
    "variant_2 = send_prompt(prompt=\"Variant 2...\", **test_params)\n",
    "\n",
    "rprint(\"[green]✓[/green] Same parameters ensure fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Success Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response, criteria):\n",
    "    metrics = calculate_metrics(response)\n",
    "    tone = analyze_tone(response['text'])\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    if criteria.get('max_words') and metrics['word_count'] <= criteria['max_words']:\n",
    "        score += 1\n",
    "    \n",
    "    if criteria.get('formality') and tone['formality'] == criteria['formality']:\n",
    "        score += 1\n",
    "    \n",
    "    if criteria.get('max_cost') and metrics['estimated_cost'] <= criteria['max_cost']:\n",
    "        score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "criteria = {\n",
    "    'max_words': 100,\n",
    "    'formality': 'formal',\n",
    "    'max_cost': 0.01\n",
    "}\n",
    "\n",
    "rprint(\"[green]✓[/green] Defined evaluation criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Multiple Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_a = \"Explain quantum entanglement briefly.\"\n",
    "prompt_b = \"Explain quantum entanglement in 2-3 sentences.\"\n",
    "\n",
    "runs = 3\n",
    "results_a = []\n",
    "results_b = []\n",
    "\n",
    "for i in range(runs):\n",
    "    resp_a = send_prompt(prompt=prompt_a, temperature=0.7, client=client)\n",
    "    resp_b = send_prompt(prompt=prompt_b, temperature=0.7, client=client)\n",
    "    results_a.append(calculate_metrics(resp_a))\n",
    "    results_b.append(calculate_metrics(resp_b))\n",
    "\n",
    "avg_words_a = sum(r['word_count'] for r in results_a) / runs\n",
    "avg_words_b = sum(r['word_count'] for r in results_b) / runs\n",
    "\n",
    "rprint(f\"\\nAverage words - A: {avg_words_a:.1f}, B: {avg_words_b:.1f}\")\n",
    "rprint(f\"[green]Winner:[/green] {'A' if avg_words_a < avg_words_b else 'B'} (more concise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "- ✓ Running basic A/B tests\n",
    "- ✓ Comparing multiple variants (A/B/C/D)\n",
    "- ✓ Analyzing metrics and tone\n",
    "- ✓ Visualizing comparisons\n",
    "- ✓ Testing different aspects (structure, temperature)\n",
    "- ✓ Best practices for reliable testing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **04_batch_processing.ipynb**: Scale your A/B tests\n",
    "- **05_evaluation_metrics.ipynb**: Define custom success metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
