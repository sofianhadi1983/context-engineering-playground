{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Batch Processing\n\nLearn how to efficiently process multiple prompts and handle large-scale operations.\n\n## What You'll Learn\n- Processing multiple inputs efficiently\n- Batch API operations\n- Result aggregation and analysis\n- Error handling strategies"]
  },
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from prompt_playground.client import create_client, send_batch\nfrom prompt_playground.analysis import compare_responses\nfrom prompt_playground.cache import ResponseCache\nfrom rich import print as rprint\nimport pandas as pd\n\nclient = create_client()\ncache = ResponseCache(ttl=3600)\nrprint('[green]✓[/green] Setup complete')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Basic Batch Processing"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["topics = ['Python', 'JavaScript', 'Rust', 'Go', 'TypeScript']\nprompts = [f'Explain {topic} in one sentence.' for topic in topics]\n\nresponses = send_batch(prompts=prompts, client=client)\n\nfor topic, response in zip(topics, responses):\n    rprint(f'\\n[cyan]{topic}:[/cyan] {response[\"text\"]}')\n    rprint(f'Tokens: {response[\"output_tokens\"]}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Using Templates for Batch Operations"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from prompt_playground.prompts import PromptTemplate\n\ntemplate = PromptTemplate(\n    template='Translate \"{text}\" to {language}.',\n    variables=['text', 'language']\n)\n\ntranslations = [\n    {'text': 'Hello world', 'language': 'Spanish'},\n    {'text': 'Hello world', 'language': 'French'},\n    {'text': 'Hello world', 'language': 'German'},\n]\n\nprompts = [template.fill(**t) for t in translations]\nresponses = send_batch(prompts=prompts, temperature=0.3, client=client)\n\nfor trans, resp in zip(translations, responses):\n    rprint(f'{trans[\"language\"]}: {resp[\"text\"]}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Caching for Efficiency"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n\nprompt = 'What is machine learning?'\nmodel = 'claude-sonnet-4-5-20250929'\nparams = {'temperature': 1.0, 'max_tokens': 100}\n\nstart = time.time()\ncached = cache.get(prompt, model, params)\nif cached:\n    rprint('[green]Cache hit![/green]')\nelse:\n    from prompt_playground.client import send_prompt\n    response = send_prompt(prompt=prompt, **params, client=client)\n    cache.set(prompt, model, params, response)\n    rprint('[yellow]Cache miss - stored[/yellow]')\nend = time.time()\n\nrprint(f'Time: {(end-start)*1000:.2f}ms')\nstats = cache.get_stats()\nrprint(f'Hit rate: {stats[\"hit_rate_percent\"]}%')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Error Handling in Batches"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_batch_safe(prompts, client):\n    results = []\n    errors = []\n    \n    for i, prompt in enumerate(prompts):\n        try:\n            response = send_prompt(prompt=prompt, client=client)\n            results.append({'index': i, 'success': True, 'response': response})\n        except Exception as e:\n            errors.append({'index': i, 'error': str(e)})\n            results.append({'index': i, 'success': False, 'error': str(e)})\n    \n    return results, errors\n\ntest_prompts = ['Valid prompt', 'Another valid one']\nresults, errors = process_batch_safe(test_prompts, client)\n\nrprint(f'Success: {sum(1 for r in results if r[\"success\"])}/{len(prompts)}')\nrprint(f'Errors: {len(errors)}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Aggregating Results"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prompts = [f'Write a {word} sentence.' for word in ['short', 'medium', 'long']]\nresponses = send_batch(prompts=prompts, client=client)\n\ndf = compare_responses(responses)\ndf['type'] = ['short', 'medium', 'long']\n\nrprint('\\n[bold]Summary:[/bold]')\nrprint(f'Total tokens: {df[\"total_tokens\"].sum()}')\nrprint(f'Total cost: ${df[\"estimated_cost\"].sum():.6f}')\nrprint(f'Avg words: {df[\"word_count\"].mean():.1f}')\n\ndisplay(df[['type', 'word_count', 'output_tokens', 'estimated_cost']])"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Best Practices\n\n### 1. Use Appropriate Batch Sizes"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_in_chunks(prompts, chunk_size=10):\n    results = []\n    for i in range(0, len(prompts), chunk_size):\n        chunk = prompts[i:i+chunk_size]\n        responses = send_batch(prompts=chunk, client=client)\n        results.extend(responses)\n        rprint(f'Processed {min(i+chunk_size, len(prompts))}/{len(prompts)}')\n    return results\n\nrprint('[green]✓[/green] Chunk processing prevents timeouts')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Monitor Costs"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def batch_with_budget(prompts, max_cost, client):\n    total_cost = 0\n    results = []\n    \n    for prompt in prompts:\n        if total_cost >= max_cost:\n            rprint(f'[yellow]Budget limit reached: ${total_cost:.6f}[/yellow]')\n            break\n        \n        response = send_prompt(prompt=prompt, client=client)\n        from prompt_playground.client import estimate_cost\n        cost = estimate_cost(response['input_tokens'], response['output_tokens'], response['model'])\n        total_cost += cost['total_cost']\n        results.append(response)\n    \n    rprint(f'[green]Processed {len(results)} prompts for ${total_cost:.6f}[/green]')\n    return results"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n\nYou've learned:\n- ✓ Batch processing basics\n- ✓ Using templates for scale\n- ✓ Caching for efficiency\n- ✓ Error handling strategies\n- ✓ Result aggregation\n- ✓ Cost management\n\n## Next Steps\n\n- **05_evaluation_metrics.ipynb**: Evaluate batch results"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
